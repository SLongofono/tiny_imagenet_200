function [net, info] = mass_train_cifar_lenet(varargin)
% This is the Cifar_Lenet architecture as given by matconvnet, repurposed for
% training 10-class subsets of tiny-imagenet-200.
%
% You must run generate_TINY_IMAGENET.m in the datasets folder with the proper
% arguments in order to prepare the images for this network.

% default optional argument settings
opts.numEpochs = -1;

% Optionally change hyperparameters
opts.weightDecay = 0.0001 ;
opts.batchSize = 100 ;
opts.lr1 = 0.05 ;
opts.lr2 = 0.005 ;
opts.lr3 = 0.0005 ;
% Organization paths
opts.train_output_path = '';
opts.imdb_path = '';

% parsing optional arguments
opts = vl_argparse(opts, varargin);

% load settings to get imdb path
SystemSettings;

[net, info] = train.mass_train_helper(@init_net, ...
    opts.train_output_path, opts.imdb_path, ...
    'numEpochs', opts.numEpochs, 'weightDecay', opts.weightDecay, 'batchSize', opts.batchSize, 'lr1', opts.lr1, 'lr2', opts.lr2, 'lr3', opts.lr3);

function net = init_net(weightDecay, batchSize, lr1, lr2, lr3)
% This function was taken from examples/cifar/cnn_cifar_init.m in
% matconvnet-1.0-beta20

lr = [.1 2] ;

% Define network 
net.layers = {} ;

% Block 1
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.01*randn(5,5,3,32, 'single'), zeros(1,32, 'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ;
net.layers{end+1} = struct('type', 'relu') ;

% Block 2
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(5,5,32,32, 'single'), zeros(1,32,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'avg', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ;

% Block 3
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(5,5,32,64, 'single'), zeros(1,64,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('type', 'relu') ;
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'avg', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ; % Emulate caffe

% Block 4
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(4,4,64,64, 'single'), zeros(1,64,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 0) ;
net.layers{end+1} = struct('type', 'relu') ;

% Block 5
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{0.05*randn(1,1,64,10, 'single'), zeros(1,10,'single')}}, ...
                           'learningRate', .1*lr, ...
                           'stride', 1, ...
                           'pad', 0) ;

% Loss layer
net.layers{end+1} = struct('type', 'softmaxloss') ;

% Meta parameters
net.meta.inputSize = [32 32 3] ;
net.meta.trainOpts.learningRate = [lr1*ones(1,30) lr2*ones(1,10) lr3*ones(1,5)] ;
net.meta.trainOpts.weightDecay = weightDecay ;
net.meta.trainOpts.batchSize = batchSize ;
net.meta.trainOpts.numEpochs = 45 ; %numel(net.meta.trainOpts.learningRate) ;
% Everything else defined in train.mass_train_helper

% Fill in default values
net = vl_simplenn_tidy(net) ;
